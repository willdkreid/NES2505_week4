---
title: 'Practical 4: Generalised Linear Models'
author: "Roy Sanderson"
output:
  html_document:
    df_print: paged
  word_document: default
---
# Introduction
Generalised linear models (GLMs) are, as their name suggests, an extension of the standard linear model. They still work to the same framework of a response variable, and one or more explanatory variables. The difference is that they can be much more flexible in the type of response variable you are working with. In a conventional linear model it is assumed that your response variable can take any value, e.g. 4.65, 1.98, -4.32, 65.0, 9241.2 etc. These represent a "normal" distribution or bell-shaped curve; you will also see a normal distribution called a "Gaussian" distribution, after the German mathematician Carl Gauss who first described it. Of course in "real" biological data, some of these values would not make any sense. You might have:

* **Binary data** Presence / Absence, Dead / Alive, Diseased / Healthy, Mutant / Wild Type etc.
* **Count data** Number of birds in different gardens, counts of pollinators visiting flowers, number of muscle reflexes in response to electical stimuli etc.
* **Positive numbers only** Heights of oak trees, weights of brown rats, yield of crops etc. These numbers might have decimal places, but they cannot be negative
* **Proportion or percentages** These are "bounded" numbers, between 0 and 1.0, or 0 and 100 respectively.

If you use a conventional linear model with these types of data, when you check the model assumptions, especially via a QQ plot, you may observe distortions, suggesting the linear model is not valid. You can sometimes transform your data before analysis by a linear model, for example take the `log(number_of_birds + 1)` as a response variable (we include the `+ 1` as it is not possible to take a logarithm of zero). However, it is usually better to use a generalised linear model from the outset.

A linear model works best with normal (Gaussian) data, but a GLM can be extended to a wide range of data types, including all four described above. To remind yourself of the underlying theory of GLMs go to [This Interactive Website](https://naturalandenvironmentalscience.shinyapps.io/generalised_linear_models/) and we will begin by using some of the examples from that website.

The overall aim of this practical is to help you have a better understanding of GLMs, how to set them up in R/RStudio, and learn how to interpret their outputs. Specific objectives of Practical 4 are:

1. Use of GLMs with counts (whole number response data)
2. Use of GLMs with binary responses (presence / absence, dead / alive etc.)
3. Explore these concepts with new datasets
4. Consider lognormal response data (response positive numbers, e.g. weight, height)

## Setting up for the practical  

First, go to Canvas and download the **lichen.csv**,  **sppcount.csv** and **beetle.csv** data sets and save them to your **Data** folder in the **NES2303** folder where you have your R project stored.  

Second, start RStudio, and click on **File -> Recent Projects** and select the NES2303 project.

Finally, create a new R script by clicking on **File -> New File -> R Script**. Save this file into your NES2303 folder, calling it _**Practical_4.R**_.

## Initial lines in your script

As usual, begin your script with a useful comment, and then load the `mosaic` and `car` packages.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Practical 4. Exploration of data using generalised linear models

# Load up essential libraries before beginning
library(mosaic)
library(car)

```

#### Importing data from .csv files into R 
Now load in the two .csv files you downloaded from  Canvas. `lichen.csv` and `sppcount.csv` at the very beginning of the practical. **Note**: Remember to move the two files into your **`Data`** folder before using `read.csv()` to import them into R.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
lichen_dat <- read.csv("Data/lichen.csv")
sppcount_dat <- read.csv("Data/sppcount.csv")
beetle_dat <- read.csv("Data/beetle.csv")
```


# 1. GLMs with counts data
## 1.1 Counts of numbers of aquatic invertebrate species
Aquatic invertebrates provide very useful bioindicators of river systems. A pollution event might be short-lived, and pollutants in the water only detectable for a few days after a spillage. However the effects on the aquatic invertebrates can be detectable for days, weeks or months. They are used by the UK's Environment Agency, Defra, Scottish, Welsh and NI governments to assess aquatic pollution through a system known as RIVPACS [see their website here for more details](https://www.ceh.ac.uk/services/rivpacs-reference-database).

## 1.2 Summary Statistics and visualisation of aquatic invertebrate data
It is always good to start by observing the data and running some summary statistics. The data are are two columns of data, recording both the numbers of aquatic invertebrate species, and the water pollution levels, at 30 rivers.

```{r echo=TRUE, include=TRUE}
# Check the top of your data, and overall summary
head(sppcount_dat)
summary(sppcount_dat)
str(sppcount_dat)
```

After looking at the data, what do you think is the response and what is the explanatory variable? **Hint**: think about "cause" and "effect". When plotting your data, what would go on your x-axis, and which variable on your y-axis. Notice also that the `str()` function which returns the "structure" of the data, records `pollution` as numeric (`num`) and species as integer (`int`). An integer is a whole number, i.e. 6, 8, 12 with no decimal points.

```{r}
mean(~pollution, data=sppcount_dat) # Overall average pollution
mean(~species, data=sppcount_dat)   # Overall average number of species at all the rivers
sd(~species, data = sppcount_dat)   # Overall standard deviation of the species number
sd(~pollution, data = sppcount_dat) # Overall standard deviation of the pollution
```

Notice that in the above commands we are calculating the overall mean and sd for species count and pollution. Thus the format of the call can be `mean(~pollution, data=sppcount_dat)`. Why might `mean(species ~ pollution, data=sppcount_dat)` **not** be appropriate to work out the average number of species at each pollution level? **Hint:** Is pollution a continuous or categorical explanatory variable? Try the command and see what you get.

Now let's visualise the data. Recall your graph-plotting skills from the previous practicals, and try and create a scatter plot similar to the following:

**Hints**

* Build up your scatter plot one line at a time, with `gf_point()` the first line, with the correct formula and dataset specified. Make sure in your formula you have the response (left-hand side of `~` symbol) and explanatory variable the right way round.
* Use the `%>%` or "THEN" symbol at the end of the first and subsequent lines before adding new ones
* On your second line, refine your labels with `gf_labs()`
* On your third line, add a fitted linear model line using `gf_lm()`. The plot below also includes the 95% confidence intervals for this fitted line. Use `?gf_lm` to read the help page to work out how to use the `interval` option to the function
* finally `gf_refine()` to refine your graph using a different theme such as `theme_bw()` or as here `theme_classic()`


```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Visualise with scatter points

gf_point(species~pollution, data = sppcount_dat) %>% 
  gf_labs(x = "Aquatic pollution level (mg/l)", y="Number of invertebrate species") %>% 
  gf_lm(interval = "confidence") %>% 
  gf_theme(theme_classic())

```

The plot above shows a clear link between the total number of aquatic invertebrate species and the amount of pollution present in the water. Visualisation is insufficient to test our (null) hypothesis that there about relationships between pollution and number of species. We need a formal statistical model. We will begin by doing it the "wrong" way with a linear model, and diagnose what is incorrect. **Note**: Look carrefully at the graph you have just created, and you might already be able to detect a potential problem. What is it?

## 1.3 Incorrect analysis via linear model
For the linear model we simply set our count of the number of species as the response, and pollution level as the explanatory in the usual way. Go ahead and store the results of your `lm()` call in an R object called `sppcount_lm`, and produce a summary of the results. Check that you get the following:


```{r, echo=FALSE}
sppcount_lm <- lm(species ~ pollution, data=sppcount_dat)
```
  
```{r echo=FALSE}
summary(sppcount_lm)

```

As you would expect, the model is highly significant, with a negative (significant) gradient for slope of -2.57, in other words for every 1 mg/l increase in pollution there is a decline in the number of species by about 2.57. When there is no pollution the number of species is about 29, which matches the graph you have just plotted above, when the horizontal axis of pollution is at zero.  There are two major problems however, the second of which you may already have spotted:

### 1.3.1 Have model assumptions been broken?
You will recall that linear models assume your data (or more precisely, the noise or errors around your fitted line) come from a normal or Gaussian distribution. The easiest way to check this is via a QQ plot:

```{r}
# Check that assumptions of model are correct - no they are not!

plot(sppcount_lm, which = 1:2)
```

The plot of residuals vs fitted looks reasonable, although the scatter of points is
smaller at low fitted values than desirable. The main problem is that there is a strong S-bend in the QQ plot, hinting that there might be problems with the model and its assumptions are not satisfied.

### 1.3.1 What are the predicted numbers of species in heavily polluted rivers?
Hopefully you have already detected that this is a fundamental problem, simply by looking at your original graph of the data, with the straight line of the number of species added. What, for example, is the predicted number of species when water pollution is at 12 mg/l ?  Let's work it out:


```{r}
# Create a predictor function
lm_species_predictor <- makeFun(sppcount_lm)

# Obtain predicted value at 12 mg/l
lm_species_predictor(12)

```

Uh oh! We seem to be predicting a **negative** number of species, but we know that this is biological nonsense. The minimum value we should get is zero.

## 1.4 GLM and Simeon Poisson to the rescue...
GLMs are much more flexible, and we can use them to produce a much better and biologically meaningful model. Simeon Poisson was an early 19th Century French mathematician devised a distribution that handled count rather than continuous data. See [This interactive website](https://naturalandenvironmentalscience.shinyapps.io/generalised_linear_models/#section-poisson-and-binomial-distributions) to view the Poisson distribution. When the website loads, the distribution initially looks like a normal distribution, but with whole numbers. It has one parameter $\lambda$ (Greek letter lambda). Move the slider. You will notice that the distribution never contains values less than zero.

Let's now change from a linear model to a generalised linear model. If we simply replace `lm()` with `glm()` we will actually get the same results as before, because it defaults to using a normal distribution. However, if we use the `family` option we can specify that we want a Poisson distribution. Try the following and check your results match:


```{r echo=TRUE, include=TRUE}
sppcount_glm <- glm(species ~ pollution, data = sppcount_dat, family = poisson(link = "log"))
summary(sppcount_glm)

```

You can see that both the intercept and the (gradient) for pollution are highly significant. You will also notice that the wording and column headings are slightly different from your linear model. Linear models use a technique known as "least squares" to estimate parameters, whereas generalised linear models use a method called "maximum likelihood", so the underlying mathematics differ.

So now what does this all mean. 

* **deviance** is the term used for variation in your data points
* The **deviance residuals** line gives a summary of the min, max, mean of all this variation after the model has been fitted, in other words a summary of the noise for all 30 rivers.
* The **coefficients** are your **explanatory** variables (i.e. pollution). You may notice that there are two coefficients when our model only specified one.

 - `(Intercept)` As usual, this is the predicted number of species when your explanatory variable (pollution) is zero. Here the estimated value for this is 3.43, which seems very low, but keep reading.
 - `pollution` this is the change in your number of species with increasing pollution, which is negative, reflecting a decline in biodiversity with pollution. However, the value of -0.139 probably seems a bit odd
- instead of t-statistics, maximum likelihood methods such as GLM produce z-statistics, which can be interpreted in a similar way to produce p-values. Both p-values are extremely low. Remember that `7.79-e13` is shorthand for `0.000000000000779` which you would write as `p<0.001` in a report.

* The **residual deviance** is a measure of how good your model is. If your residual deviance value is close to zero, then the model is not very good. 
* The **AIC** value is used when comparing two similar models, the lower the AIC, the better the model.
* Finally, the **Fisher score** is a measure of how many iterations of the model R ran looking at different estimates before displaying the results. 

There is a lot of information in the output and it is easy to get overwhelmed. The main things to note are the `Estimate` values, and when you report them in a table you would typically quote their associated z and p statistics, along with the overall AIC.

#### What about the strange Estimates from the GLM?
As we noted above, the values of 3.43 and -0.139 do not quite match what was expected. Indeed, looking at your original graph, an estimated number of species of only 3.43 when the water is unpolluted (x-axis at zero) does not match your graph where it looks to be around 29 or 30 species! This is because GLMs use what is known as a "link function" in their calculations; if you are interested see the [How to generalise page from the interactive website](https://naturalandenvironmentalscience.shinyapps.io/generalised_linear_models/#section-how-to-generalise). Poisson GLMs use natural logarithms as their link function, so we would need to take the antilogarithm to get back to the original units. As these are natural logarithms, (i.e. $log_e$ rather than $log_{10}$) we need to take the exponential to convert to the original units, available through the `exp()` function. Let's check with the expected number of species when pollution is zero, i.e. the intercept:

```{r}
exp(3.43)
```

That looks more sensible based on our original scatterplot of the data. To finish off, we will check the model assumptions, predict the number of species when pollution is 12 mg/l and replot our original graph, but with a GLM rather than linear model.

### 1.4.1 Check Poisson GLM model assumptions
As before, create a QQ plot of the residuals. Copy, paste and edit your original `gf_qq()` and `gf_line()` code in your `Practical_4.R` script to save time typing. You should end up with a graph similar to this:

```{r, echo=FALSE}
# Check GLM model assumptions - these look good
plot(sppcount_glm, which = 1:2)
```

Although the QQ plot still shows a slight S-bend in the scatter of points, it is nothing like as severe a problem as we had with the linear model of the same data, so we can be much more confident about the model.

### 1.4.2 What is the predicted number of species in heavily polluted streams with the GLM?
With the linear model we had the problem that our model could predict negative numbers of species in polluted water. What do we get with the GLM? Modify your `makeFun()` call to create a new `glm_species_predictor()` function, and see if you get this value for 12 mg/l pollution?

```{r, echo=FALSE}
# Create a predictor function
glm_species_predictor <- makeFun(sppcount_glm)

# Obtain predicted value at 12 mg/l
glm_species_predictor(12)

```

The biodiversity is obviously very low, but it is still positive, which makes more sense than the negative number of species estimated by the incorrect linear model for 12 mg/l pollution we used in Section 1.3.

### 1.4.3 Modify scatterplot of original data to show Poisson GLM
Our original scatter plot had a straight line from the `gf_lm()` function. Instead, we will add a curve from our GLM model, using `gf_smooth()`. This function has several methods for creating curves, and we specify that we used a GLM, with Poisson errors, and gives a smooth curve. **Build the following graph up slowly**, one line at a time, re-creating it each time, so you understand what each command does:

```{r}
gf_point(species ~ pollution, data=sppcount_dat) %>% 
  gf_labs(x = "Aquatic pollution level (mg/l)", y="Number of invertebrate species") %>% 
  gf_smooth(method="glm", method.args = list(family=poisson(link = "log")), se=TRUE) %>% 
  gf_refine(theme_classic())
```



## 2. GLMs with Binary Data
Your experiment may produce categorical response data, e.g. male or female, wilted or turgid, infected or uninfected, dead or alive. Note that a response such as simply 'dead' or 'alive' differs from a response where you have counts of the numbers of dead or alive out of your original sample. So binary data can have two slightly different forms:

* Simple presence / absence; dead / alive; 0 / 1 data (binary response)
* Information about sample size. e.g. number of eggs that hatch, out of egg clutches that vary slightly in their size between nests.

The latter has more information (and is proportion data) so can be analysed slightly differently, but both use the **binomial distribution**, see [this page of the Interactive website](https://naturalandenvironmentalscience.shinyapps.io/generalised_linear_models/#section-poisson-and-binomial-distributions). In the simple binary response the sample size, shown as "Size of each trial" on the interactive graph on the website, is 1. In the second type of scenario, the sample size would vary depending on for example the size of each egg clutch in different nests.

In this practical we will focus on the simpler type of data, where you just have a binary response. We will use the data `lichen_dat` that you read into R at the start of this practical. In this example, you are testing the effect of atmospheric pollution on the presence or absence of lichen on trees. Lichens have been found to be very sensitive biological indicators of atmospheric pollution. As lichen presence can be affected by the tree bark, particularly calcium content, this was also measured. Your response is a 1 or 0, where 1 indicates lichen are present and 0 lichen absent. 

## 2.1 Summary statistics and visualisation of lichen data

Start with some visualisation and summary statistics. What do the plots bellow tell you about lichen ecology? Can you form some initial assumptions based on what they show?  


```{r echo=TRUE, include=TRUE}
summary(lichen_dat)
str(lichen_dat)
```

Before we continue, notice that calcium and air pollution are continuous variables, indicated by `num` in the structure of the data. However `lichen` is showing as a `int` or whole-number integer, and the summary is giving a mean value of 0.58 for the lichens. This is a little misleading, as really the lichens are a categorical binary response. We can force R to recognise this by using the `as.factor()` function. Remember we use the `$` symbol to allow us to refer to a single column:

```{r}
lichen_dat$lichen_fct <- as.factor(lichen_dat$lichen)
summary(lichen_dat)
str(lichen_dat)
```

You can see that the `summary` function has a new variable called `lichen_fct` which will be more convenient for some plots. Let's begin with a scatter-plot of the observed points against the two explanatory variables:

```{r}
# Scatter plots of lichens vs calcium or air pollution
gf_point(lichen ~ calcium, data=lichen_dat) 
gf_point(lichen ~ airpoll, data=lichen_dat)
```

The problem with these two plots is whilst you can see that there appears to be more lichens when at higher calcium and lower air pollution, the plots are actually of limited value for exploratory data visualisation. Boxplots or violinplots would be more useful. Of course, by default they place the categorical explanatory variable on the horizontal axis, and the continuous response variable on the vertical axis. But here **our response is categorical (presence or absence of lichen)**, so really we want lichen incidence on the vertical y-axis, and calcium or air pollution on the horizontal x-axis.

We can get round this problem by flipping our boxplots or violin plots through 90-degrees, so that the lichens still appear on the vertical axis, as we would expect for a response variable. We can do this with the `coord_flip()` option to `gf_refine()`. Both boxplots and violinplots expect categorical variables (factors) so we will use `lichen_fct` in their call.

Let's create a simple boxplot first to show the idea. We use the `lichen_fct` as the response, rather than `lichen` as we want a categorical variable (lichen presence or absence) in a boxplot. In previous boxplots your response has been continuous, and your explanatory categorical. However, here we put it the other way round, to reflect the biology:

```{r}
gf_boxplot(lichen_fct ~ calcium, data=lichen_dat) 
```

Now create a similar boxplot, but showing the differences in lichen response to air pollution. You may wish to smarten up your plot to improve the axis titles etc. The `gf_jitter` function allows you to see the original data points:

```{r}
gf_boxplot(lichen_fct ~ airpoll, data = lichen_dat) %>% 
  gf_jitter() %>% 
  gf_labs(x = "Air pollution", y = "Lichen incidence") %>% 
  gf_theme(theme_classic()) 
```

Displaying your data this way gives you a much better insight into the relationships between the response variable of lichen, and explanatory of air pollution or calcium, whilst still adhering to the convention of placing your **response on the vertical** axis, and the **explanatory on the horizontal** axis.

## 2.2 GLMs with binary response variable
### 2.2.1 Interaction terms in linear models and generalised linear models
Here we have two explanatory variables, calcium and air pollution. We cannot assume that they are independent in their effects, and they may **interact** in some way in terms of their impact on the presence or absence of lichens. You have already covered interaction terms in Practical 3, and remind yourself of how they work at [interactive website explaining interaction terms](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-interactions-between-explanatory-variables)

When a GLM contains a `:` in the explanatory variables, such as

`glm(response ~ explan1 + explan2 + explan1:explan2, data = data, family = family)`

it means that the model will test to see if an interaction between the two variables is having a significant effect. It also tests the individual "main effects" of `explan` and `explan2`. There is also a shortcut for the above structure in that if you use `*` all the main effects and interactions will be tested. Thus:

`glm(response ~ explan1 + explan2 + explan1:explan2, data = data, family = family)`

and

`glm(response ~ explan1*explan2, data = data, family = family)`

are identical. The latter type of structure to call either linear or generalised linear models with interactions can save you having to type all the main effects separately, and is especially useful with e.g. 3 (or more) explanatory variables.

We will begin by running a full model including the interaction terms; however, these can be removed later if not necessary. Notice that the family for this model has been changed as we now have a binary response variable. The appropriate family to describe the error distribution when you have presence/absence data is the **binomial** distribution. Go to [interactive website](https://naturalandenvironmentalscience.shinyapps.io/generalised_linear_models/#section-poisson-and-binomial-distributions) to remind yourself of a binomial distribution. Your sample size here is 1 as there is only a single observation at each site as to whether lichen is present or absent.

```{r echo=TRUE, include=TRUE}

lichen_glm1 <- glm(lichen ~ airpoll + calcium + airpoll:calcium, data = lichen_dat, family = binomial(link = "logit"))
summary(lichen_glm1)

```

The output is very similar to the previous GLM, but here we have four parameters estimated, the intercept, the two "main effects" for air pollution and calcium, and the interaction term. You can see that all four are non-significant, so you might be tempted to assume there is nothing going on. However:

* Think back to your violinplots (or boxplots) of the lichens and these environmental variables. These seem to suggest some sort of relationship.
* Always look at the interaction term first. If it is non-significant, refit the model without the interaction term.

### 2.2.2 Two ways to update your (linear or) generalised linear model
When you have the results of a model with one or more interaction terms, the best strategy is to:

* refit the model without the most complicated non-significant interaction term (here simply the `airpoll:calcium` term)
* look at the `summary()` of the model
* see if the simpler model is significantly different from the more complex one using `anova()`
* repeat if needed

There are actually two ways available to refit your model without the interaction term, and you can use both approaches for models fit with either `lm()` or `glm()`. The first is fairly obvious:

```{r, eval=FALSE}
lichen_glm2 <- glm(lichen ~ airpoll + calcium, data = lichen_dat, family = binomial)
summary(lichen_glm2)
```

The second "shorthand" method is useful if you start off with lots of interaction terms and gradually want to simplify your model, as it saves the amount you need to type. When you have 3 interaction terms you actually estimate lots of parameters:

1. Intercept
2. Explan1 (main effect)
3. Explan2 (main effect)
4. Explan3 (main effect)
5. Explan1:Explan2 (2-way interaction)
6. Explan1:Explan3 (2-way interaction)
7. Explan2:Explan3 (2-way interaction)
8. Explan1:Explan2:Explan3 (3-way interaction)

The situation gets worse the more explanatory variables you have. So if you need to simplify your model, having to explicitly retype all the possible terms to keep can easily result in typing errors. Here it is not a major problem, as you only have one interaction term to worry about. The trick is to `update()` your original model, and remove the non-significant interaction term:

```{r}
lichen_glm2 <- update(lichen_glm1, . ~ . -airpoll:calcium)
```

The syntax of the `update()` function can be a little confusing, so let's break it down:

* `lichen_glm1` this was our original "full" model, that we want to simplify
* `. ~ .` This is the `response ~ explanatory` equation. You are probably wondering why the variable names are not specified. By simply putting a `.` symbol to the left and right of the `~` symbol, you are telling `update()` to begin with the same set of response and explanatory variables as are defined in `lichen_glm1`
* `-airpoll:calcium`This now tells `update()` to **delete the interaction term** as it has a minus symbol in front of it.

If you have a complex `lm()` or `glm()` model that would take you several attempts to simplify, the `update()` function can save on typing. Now check the results of the simplified GLM model:

```{r, echo=FALSE}
summary(lichen_glm2)
```

Notice now that the main effects for air pollution and calcium are significant after removing the interaction term. Also, the **AIC** value is lower in the second model meaning this model is an improvement over the more complex one we ran earlier. What do the results of this simpler model tell you about lichen and its relationship with calcium and air pollution?

You might want to know if this simpler model is a significant improvement on the first, and you can use the `anova()` function to do this. As you know, by default `anova()` calculates "sums of squares", "mean squares" (old-fashioned name for variances) and "F-ratios" as part of the process of testing for significance to calculate a p-value. However, GLMs do not use mean squares, instead they use "maximum likelihood" to estimate parameters. Therefore we have to tell the `anova()` function to do a Chi-squared test to see if the two models are the same:

`````{r, eval=FALSE}
anova(lichen_glm1, lichen_glm2, test="Chisq")
```
```{r, echo=FALSE}
print(anova(lichen_glm1, lichen_glm2, test="Chisq"))
```

At first this is a bit confusing; it says there is no difference (p greater than 0.05) in the models! The way to interpret this is that the second, simpler model, is just as effective as the overly complicated one that contained the interaction term. The second model is simpler, has a lower AIC indicating it is better, and is just as effective, so is the one to use.

### 2.2.3 Checking binomial model assumptions and making predictions
It is actually quite difficult to interpret the residuals from a binomial model, because the response variable is binary. Therefore, whilst you can create a QQ plot, do not be surprised if the plot shows quite a big curve in it. Some people recommend the `binnedplot()` function from the `arm` R package, which we have not used in this course. If you wish, feel free to investigate it, but it is entirely optional.

You can of course make a prediction from your model. First, let us produce a plot of the predicted probability of lichen at different calcium levels. To avoid the problem of all the points for presence or absence of lichen overlapping each other, we will use `gf_jitter()` instead of `gf_point()`. The `height` option controls how much "jitter" there is around the points. The `gf_smooth()` function is again used to add the fitted line, but now we have to specify that the `binomial` error family is used. As usual, build up the plot one line at a time, to see what happens, adding a `%>%` symbol at the end of the preceding line when you want to add a new one:

```{r}
gf_jitter(lichen ~ calcium, data=lichen_dat, height=0.05) %>% 
  gf_smooth(method="glm", method.args = list(family=binomial(link = "logit")), se=TRUE)
```

Add additional R lines to change the overall plotting theme, modify the axis labels, and repeat for air pollution.

Now use the `makeFun()` approach with `lichen_glm2` to predict the probability of lichens being present at a calcium level of 2, and air pollution of 5. **Remember** as you have two variables in your model, you will need to enter both variables into your `lichen_glm_predict()` command. You should obtain the following value. I have used the `round()` function to round to 4 decimal places:

```{r, echo=FALSE}
lichen_glm_predict <- makeFun(lichen_glm2)
round(lichen_glm_predict(calcium=2, airpoll=5), 4)
```


# 3. Binomial GLMs where the sample size varies
## 3.1 Sample sizes in binomial GLMs
The lichen example had a sample size of one, in that one record (presence or absence of lichen) was available at each site. However, if you look at the [interactive description of binomial distributions](https://naturalandenvironmentalscience.shinyapps.io/generalised_linear_models/#section-poisson-and-binomial-distributions) you can see that we can we can in fact vary the sample size. Initially on the website it is set a 1, but you can change this and see what happens.

We can take this extra information into account when doing the analyses. The `beetle_dat` dataset you imported at the start shows the numbers of flour beetles that are killed with different does of the insecticide carbon disulphide (mg/l). View the data using the usual R commands to get an understanding of it. It is no surpise to see that the mortality rate increases with pesticide dose. The `Mortality_rate` has been calculated as `Number_killed` divided by `Number_tested`. See if you can create a plot similar to the following:

```{r}
gf_point(Mortality_rate ~ Dose, data = beetle_dat) %>% 
  gf_labs(x="Carbon disulphide concentration (mg/l)", y="Mortality rate") %>% 
  gf_refine(theme_classic())
```

Whilst it might be tempting to try and fit a curve immediately to these mortality rates, you actually have more information at your disposal: the sample size (`Number_tested`). If the sample size is big then any estimated mortality derived from it is likely to be more reliable, compared to a smaller sample size. You can see from the raw table of data in `beetle_dat` that your smallest sample size (49) received the lowest dose of insecticide, whilst the largest (63) received an intermediate dose. You can take this information into account to improve your GLM.

## 3.2 How to include sample size in your GLM
R expects to receive **two** columns in your response variable, one the number of beetles that died, the other the number that survived. We don't actually have that second column, but we can easily calculate it and include it in your `beetle_dat` table. Remember that the `$` is used to allow you to reference individual columns of the table of data:

```{r}
beetle_dat$Number_alive <- beetle_dat$Number_tested - beetle_dat$Number_killed
beetle_dat
```

You can now bind the two columns of `Number_killed` and `Number_alive` together using the `cbind()` function, and present this as your response variable to the `glm()` function:

```{r}
beetle_glm <- glm(cbind(Number_killed, Number_alive) ~ Dose, family=binomial(link = "logit"), data=beetle_dat)
summary(beetle_glm)

```

The output shows, as expected, a highly significant effect of the pesticide on the survivorship of the beetles. Again, you might be puzzled by the `-14.578` value for the Intercept, which is the mortality at zero dose of insecticide. This is because binomial GLMs use the "logit" link function which are natural logarithms of the odds of success to failure. This can be calculated manually (which is a bit tricky) so it is much easier to use the `makeFun()` approach:

```{r}
beetle_glm_predict <- makeFun(beetle_glm)
beetle_glm_predict(0)
```

Remember that when you see 4.664753e-07 it is 0.0000004664753, so the mortality rate at zero pesticide is, as expected, negligible.

## 3.3 Plotting GLM results when sample size varies
The default `gf_smooth()` assumes the sample size is the same for every point. So the best approach is to predict your mortality rate using the `beetle_glm_predict()` function for the `Dose` in your dataset, and then base your smoothed curve on that. You have to specify in your `gf_smooth()` function that you are using these predicted values. You may get a warning about non-integer values, but ignore this.

```{r, warning=FALSE}
# Add a column with predicted mortality, weighted by sample size
beetle_dat$Predicted_mortality <- beetle_glm_predict(beetle_dat$Dose)
beetle_dat

# Plot graph. You need to include the equation for gf_smooth as it uses predicted values
gf_point(Mortality_rate ~ Dose, data=beetle_dat) %>% 
  gf_smooth(Predicted_mortality ~ Dose, data=beetle_dat, method="glm", method.args = list(family=binomial(link = "logit")))
```

# Take-home messages
Generalised linear models provide a simple extension to the linear models you've already been working with. They are much more flexible, and hence a little trickier to use initially. However, many types of biological or zoological data do not fit neatly into the expected format for a linear model, and GLMs will help you. In this practical we have only looked at Poisson and Binomial errors: there are many other types available if these are unsuitable.